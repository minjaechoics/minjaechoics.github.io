## üîπ Greedy Decoding

**Core Idea**  
Continuously select the token with the highest probability  

$x_{L+1}=arg$ $max$ $\hat{p}(x)$  

**Pros**  
- Easy to use  

**Cons**  
- It can be suboptimal depending on succeeding generations  


## üîπ Beam Search

**Core Idea**  
Generate tokens over multiple possible candidate tokens  

- Allows consideration of multiple future steps for each candidate (multiple branches can be considered)  
- Token selection criterion during updates: (vanila) likelihood  
  $p(x)=p(x_1)p(x_2|x_1)...$  

**Pros**  
- Has a chance to generate better results  

**Cons**  
- High computational cost  


## üîπ Sampling

**Core Idea**  
Select words probabilistically based on the generated probability distribution  
(Existing Greedy Decoding and Beam Search *deterministically* select high-probability tokens)  

- Asking the same question to an LLM can yield different answers  

**Pros**  
- Enables more diverse cases during token generation  

**Cons**  
- The quality aspect of token generation may deteriorate  

---

### ‚ùì How can we overcome the disadvantages of Sampling Algorithms?


### (1) Temperature  
Intentionally control the model‚Äôs behavior  

Example)  
$\hat{p}(x)=softmax(\hat{o}(x)/T)$  
// If T is large, more diverse answers can be generated  


### (2) Top-K  
Since the problem lies in low-probability tokens anyway,  
let‚Äôs only consider the top k tokens with high probability!  

- Reduces user intervention  


### (3) Top-P (Nucleus Sampling)  
Starting from tokens with high generation probability,  
only consider tokens whose cumulative probability does not exceed the threshold P!  

- Do not focus on a fixed number ‚ÄòK‚Äô  
- üëâ Currently the most commonly used algorithm  

---

When customizing LLMs for various purposes such as drug discovery, retraining the entire LLM is too costly. Therefore, algorithms currently being researched focus on improving decoding algorithms to build LLMs tailored to industrial demands, as shown below.

---

## Decoding Algorithms for Increasing Diversity

### üî∏ Diverse Beam Search

**Existing Method**  
Existing Beam Search: Maintain the top K beams with the highest probabilities  

**Problem**  
Focusing only on the probability distribution results in many overlapping tokens, leading to low diversity!  

**Improvement**  
When selecting beams,  
do not only consider probability, but also impose a penalty  
based on similarity with previously generated candidates!  

$Y^{g}_{[t]}=argmax_{y^{g}_{1, [t]}, ..., y^{g}_{B', [t]}}{\varSigma}_{b\in[B']}\Theta(Y^g_{b,[t]})+\varSigma^{g-1}_{h=1}\lambda_g\Delta(y^g_{b,[t]},Y^h_{[t]})$  

$s.t.y^{g}_{i, [t]}\neq y^{g}_{j, [t]}, \lambda_g\leq0$  

---

## Decoding Algorithms for Improving Model Performance Itself

### üî∏ Contrastive Decoding

Existing problem set:  
How can we reduce small mistakes made in the middle of the process when a large model (Expert LLM) solves complex problems?  

By using a small model (Amature LM) together,  
token generation is performed from the probability distribution derived from  
"Expert LLM probability distribution" - "Amature LM probability distribution"  
($log{p_{EXP}}-log{p_{AMA}}$)  

#### ‚ö†Ô∏è How to Handle Corner Cases

*What if a token is not generated by the large model but is generated by the small model?* etc.  

**Adaptive Plausibility Constraint**

(1) The large model roughly filters tokens  
(2) The small model is applied to the filtered tokens  

$\mathrm{CD\text{-}score}(x_i; x_{<i}) = \log \frac{p_{\mathrm{EXP}}(x_i \mid x_{<i})}{p_{\mathrm{AMA}}(x_i \mid x_{<i})} \;\;\text{if } x_i \in \mathcal{V}_{\mathrm{head}}(x_{<i}), \text{ otherwise } -\infty$  


### üî∏ Visual Contrastive Decoding (VCD) (For Multimodal Models)

A method where images are converted into tokens and processed by the LLM  

Goal: Reduce hallucinations in Large Vision-Language Models (LVLMs)  

- Expert: Original image  
- Amature: Image with heavy noise applied  

---

## Decoding Algorithms for Improving Model Speed

### üî∏ Speculative Decoding

Core Idea:  
During token generation, there are positions that are easy to generate and positions that are difficult to generate!  

- e.g) The $blank$ : difficult case  
- e.g) The president was Barack $blank$ : easy case  


**(1) Draft generation using a small model (generate multiple tokens at once)**  
**(2) Pass the generated tokens to the large model**  
: "*Does the large model agree with the tokens created by the small model??*"  
**(3) The large model corrects unsuitable tokens**  

‚ùó Multiple tokens can be generated with a single use of the large model  
‚ùó Inference speed can also be improved
